{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73efd5f8",
   "metadata": {},
   "source": [
    "# Functions and Imports (no user input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11330a3a",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c141c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T21:36:28.360366Z",
     "start_time": "2025-04-21T21:35:33.322479Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import holoviews as hv\n",
    "import colorcet\n",
    "# from holoviews.operation import histogram\n",
    "from bokeh.models import HoverTool\n",
    "hv.extension('bokeh')\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "\n",
    "from IPython.display import SVG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd869f",
   "metadata": {},
   "source": [
    "## Ripley's K and L Functions (distribution of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995bb1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T21:25:14.557368Z",
     "start_time": "2025-04-21T21:25:14.557359Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def rescale_data(x):\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    return (x - min_val) / (max_val - min_val)\n",
    "\n",
    "def Ripley_K(x, scale):\n",
    "    x = rescale_data(x)\n",
    "    x_pairs = distance.cdist(x, x, 'euclidean')  # All pairwise distances\n",
    "    return np.sum(x_pairs <= scale) / len(x)\n",
    "\n",
    "def Ripley_L(x, scale):\n",
    "    x = rescale_data(x)\n",
    "    return np.sqrt(Ripley_K(x, scale) / np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb9ae8",
   "metadata": {},
   "source": [
    "## Define r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb935eac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T21:25:14.618399Z",
     "start_time": "2025-04-21T21:25:14.615447Z"
    }
   },
   "outputs": [],
   "source": [
    "# improved definiton of r2_score (https://stats.stackexchange.com/questions/590199/how-to-motivate-the-definition-of-r2-in-sklearn-metrics-r2-score) (scikitlearn uses out-of-sample y_mean)\n",
    "def r2_score(y_train, y, y_pred):  #y_train and y can be the same if determining r2 for training data (use y_train and y_validation for validation data)\n",
    "    y_bar = np.mean(y_train)\n",
    "    RSS = np.sum((y - y_pred)**2)   # Residual Sum of Squares\n",
    "    TSS = np.sum((y - y_bar)**2)    # Total Sum of Squares\n",
    "    r2_score = 1 - (RSS / TSS)\n",
    "    return r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e8f0e",
   "metadata": {},
   "source": [
    "## Determine column name and index during import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86aeb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T21:25:15.643421Z",
     "start_time": "2025-04-21T21:25:15.631070Z"
    }
   },
   "outputs": [],
   "source": [
    "## Allow for user to input both name (string) or index number for column headers\n",
    "def get_column_loc(column, dataframe):\n",
    "    if isinstance(column, str):\n",
    "        return dataframe.columns.get_loc(column), column\n",
    "    else:\n",
    "        return column, dataframe.columns[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab717d4",
   "metadata": {},
   "source": [
    "## Image generation from SMILES string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ee715",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T21:25:16.652500Z",
     "start_time": "2025-04-21T21:25:16.636680Z"
    }
   },
   "outputs": [],
   "source": [
    "# adapted from https://birdlet.github.io/2018/06/06/rdkit_svg_web/\n",
    "def DrawMol(dataframe, smiles_column_loc, image_column, molSize=(200, 100), kekulize=True):\n",
    "    images = []\n",
    "    for smiles_string in dataframe.iloc[:, smiles_column_loc]:\n",
    "        try:\n",
    "            mc = Chem.MolFromSmiles(smiles_string)\n",
    "            if kekulize:\n",
    "                try:\n",
    "                    Chem.Kekulize(mc)\n",
    "                except:\n",
    "                    mc = Chem.Mol(smiles_string.ToBinary())\n",
    "\n",
    "            if not mc.GetNumConformers():\n",
    "                Chem.rdDepictor.Compute2DCoords(mc)\n",
    "\n",
    "            drawer = rdMolDraw2D.MolDraw2DSVG(*molSize)\n",
    "            drawer.DrawMolecule(mc)\n",
    "            drawer.FinishDrawing()\n",
    "            svg = drawer.GetDrawingText().replace('svg:', '')\n",
    "            images.append(SVG(svg).data)\n",
    "        except:\n",
    "            images.append(None)\n",
    "    \n",
    "    try:\n",
    "        dataframe.insert(smiles_column_loc+1, image_column, images)\n",
    "    except: #  reason for error \n",
    "        dataframe[image_column] = images\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16cf412",
   "metadata": {},
   "source": [
    "## HoloViews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92314c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T21:25:17.749362Z",
     "start_time": "2025-04-21T21:25:17.652445Z"
    }
   },
   "outputs": [],
   "source": [
    "def scatter_plot(\n",
    "    dataframe: pd.DataFrame, \n",
    "    x: str,  # x-axis data\n",
    "    y: str,  # y-axis data\n",
    "    title: str = 'default',  # title of plot\n",
    "    x_label: str = 'default',  # axis label to be printed on plot (does not need to match dataframe name)\n",
    "    x_range: tuple = None,  # range of x-axis\n",
    "    y_label: str = 'default',  # axis label to be printed on plot (does not need to match dataframe name)\n",
    "    y_range: tuple = None,  # range of y-axis\n",
    "    legend: str = '',  # string with data label if using classifiers/building plots by category\n",
    "    svgs: str = None,  # string with column name of svgs \n",
    "    hover_list: list = None,  # list of column names with data to be shown on hover \n",
    "    marker: str = 'o',  # marker type - most of the matplotlib markers are supported (https://matplotlib.org/stable/api/markers_api.html)\n",
    "    bubbleplot: bool = False,  # if True, will create a bubble plot\n",
    "    size: int = 10,  # size of markers (recommended: 10-20)\n",
    "    bubblesize: str = None,  # string with column name for size of points in bubbleplot\n",
    "    heatmap: bool = False,  # if True, will create a heatmap\n",
    "    heatmap_col: str = '',  # color of heatmap\n",
    "    heatmap_label: str = 'default', # label for heatmap colorbar\n",
    "    heatmap_color: str = 'Plasma',  # color of heatmap\n",
    "    color: str = '#931319',  # color of markers\n",
    "    outline: str = '#29323d',  # color of marker outline\n",
    "    line_width: int = 1,  # width of marker outline\n",
    "    alpha: int = 1,  # transparency of markers\n",
    "    groupby: str = None,  # string with column name to group data by\n",
    "    height: int = 500,  #plot height (recommended: 500)\n",
    "    width: int = 500,  #plot width (recommended: 500)\n",
    "    fontscale: int = 1.2,  # scale of font size\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    scatter_plot function based off of HoloViews 'Scatter' element. See documentation for more information:\n",
    "    hv.help(hv.Scatter)\n",
    "    https://holoviews.org/reference/elements/bokeh/Scatter.html\n",
    "    \"\"\"\n",
    "\n",
    "    if x_label == 'default':  # if no x_label provided, use x column name\n",
    "        x_label = x\n",
    "    if y_label == 'default':  # if no y_label provided, use y column name\n",
    "        y_label = y\n",
    "    if heatmap_label == 'default':  # if no heatmap_label provided, use heatmap_col column name\n",
    "        heatmap_label = heatmap_col\n",
    "\n",
    "    if not x_range:\n",
    "        x_min = min(dataframe[x]); x_max = max(dataframe[x])\n",
    "        x_buffer = abs(x_max-x_min)/10\n",
    "        x_range = (x_min-x_buffer, x_max+x_buffer)\n",
    "    if not y_range:\n",
    "        y_min = min(dataframe[y]); y_max = max(dataframe[y])\n",
    "        y_buffer = abs(y_max-y_min)/10\n",
    "        y_range = (y_min-y_buffer, y_max+y_buffer)\n",
    "\n",
    "    if groupby is not None and hover_list is not None:\n",
    "        # color = hv.Cycle(color).values\n",
    "        hover_list.insert(0, groupby)\n",
    "\n",
    "    if svgs == None and hover_list == None: # no hover information provided\n",
    "        if title == 'default':  # if no title provided, define from x, y labels\n",
    "            title = f'{y_label} vs. {x_label}'\n",
    "        plt = hv.Scatter(dataframe, kdims=[x], vdims=[y], label=legend).opts(title=title, xlabel=x_label, ylabel=y_label, align='center', marker=marker, height=height, width=width, color=color, alpha=alpha, size=size, line_color=outline, line_width=line_width, fontscale=fontscale)\n",
    "    else:  # hover information provided, build list of hover tools\n",
    "        hover_list.insert(0, y)\n",
    "        tooltips = f'<div>end' # beginning of tooltips if no svgs provided\n",
    "        if svgs != None:\n",
    "            tooltips = f'<div><div>@{svgs}{{safe}}</div>end'  # beginning of tooltips if svgs are provided\n",
    "            hover_list.insert(1, svgs)\n",
    "        if len(hover_list) < 4:\n",
    "            for label in hover_list:\n",
    "                if label != svgs and label != y:\n",
    "                    tooltips = tooltips.replace('end', f'<div><span style=\"font-size: 17px; font-weight: bold;\">@{label}</span></div>end')\n",
    "        else:\n",
    "            for label in hover_list:\n",
    "                if label != svgs and label != y:\n",
    "                    tooltips = tooltips.replace('end', f'<div><span style=\"font-size: 12px;\">{label}: @{label}</span></div>end')\n",
    "        \n",
    "        tooltips = tooltips.replace('end', '</div>')\n",
    "        hover = HoverTool(tooltips=tooltips)\n",
    "        if heatmap == False and bubbleplot == False:  # if no heatmap or bubbleplot, build scatter plot  \n",
    "            if title == 'default':  # if no title provided, define from x, y labels\n",
    "                title = f'{y_label} vs. {x_label}'          \n",
    "            plt = hv.Scatter(dataframe, kdims=[x], vdims=hover_list, label=legend).opts(title=title, xlabel=x_label, ylabel=y_label, align='center', marker=marker, height=height, width=width, tools=[hover], color=color, alpha=alpha, size=size, line_color=outline, line_width=line_width, fontscale=fontscale)\n",
    "\n",
    "        elif heatmap == True and bubbleplot == False:\n",
    "            if heatmap_col not in hover_list:\n",
    "                hover_list.append(heatmap_col)\n",
    "            if title == 'default':  # if no title provided, define from x, y labels\n",
    "                title = f'{y_label} vs. {x_label}, colored by {heatmap_col}'\n",
    "            plt = hv.Scatter(dataframe, kdims=[x], vdims=hover_list, label=legend).opts(title=title, xlabel=x_label, ylabel=y_label, align='center', marker=marker, height=height, width=width, tools=[hover], color=heatmap_col, cmap=heatmap_color, colorbar=True, clabel=heatmap_label, alpha=alpha, size=size, line_color=outline, line_width=line_width, fontscale=fontscale)\n",
    "\n",
    "        elif heatmap == False and bubbleplot == True:\n",
    "            if bubblesize not in hover_list:\n",
    "                hover_list.append(bubblesize)\n",
    "            if title == 'default':  # if no title provided, define from x, y labels\n",
    "                title = f'{y_label} vs. {x_label}, sized by {bubblesize}'\n",
    "            min_size = min(dataframe[bubblesize]); max_size = max(dataframe[bubblesize])\n",
    "            plt = hv.Scatter(dataframe, kdims=[x], vdims=hover_list, label=legend).opts(title=title, xlabel=x_label, ylabel=y_label, align='center', marker=marker, height=height, width=width, tools=[hover], color=color, alpha=alpha, size=((hv.dim(bubblesize)-min_size)/(max_size-min_size)*(max_size-min_size)+min_size)*6*size, line_color=outline, line_width=line_width, fontscale=fontscale)\n",
    "\n",
    "        elif heatmap == True and bubbleplot == True:\n",
    "            if heatmap_col not in hover_list:\n",
    "                hover_list.append(heatmap_col)\n",
    "            if bubblesize not in hover_list:\n",
    "                hover_list.append(bubblesize)\n",
    "\n",
    "            if title == 'default':\n",
    "                title = f'{y_label} vs. {x_label}, colored by {heatmap_col}, sized by {bubblesize}'\n",
    "            min_size = min(dataframe[bubblesize]); max_size = max(dataframe[bubblesize])\n",
    "            plt = hv.Scatter(dataframe, kdims=[x], vdims=hover_list, label=legend).opts(title=title, xlabel=x_label, ylabel=y_label, align='center', marker=marker, height=height, width=width, tools=[hover], color=heatmap_col, cmap=heatmap_color, colorbar=True, clabel=heatmap_label, alpha=alpha, size=((hv.dim(bubblesize)-min_size)/(max_size-min_size)*(max_size-min_size)+min_size)*6*size, line_color=outline, xlim=x_range, ylim=y_range, line_width=line_width, fontscale=fontscale)\n",
    "        \n",
    "        if groupby != None:\n",
    "            # color = hv.Cycle(color).values\n",
    "            plt = plt.opts(color=groupby, cmap=color)\n",
    "\n",
    "        return plt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slope(\n",
    "    dataframe: pd.DataFrame, \n",
    "    x: str,  # string with column name, used to determine slope\n",
    "    y: str,  # string with column name, used to determine slope\n",
    "    x_label: str = 'default',  # axis label to be printed on plot (does not need to match dataframe name)\n",
    "    y_label: str = 'default',  # axis label to be printed on plot (does not need to match dataframe name)\n",
    "    color: str = '#000000',  # color of slope line\n",
    "    line_width: int = 2,  # width of slope line\n",
    "    alpha: int = 1,  # transparency of slope line\n",
    "    height: int = 500,  #plot height (recommended: 500)\n",
    "    width: int = 500  #plot width (recommended: 500)\n",
    "):\n",
    "    \n",
    "    \n",
    "    if x_label == 'default':  # if no x_label provided, use x column name\n",
    "        x_label = x\n",
    "    if y_label == 'default':  # if no y_label provided, use y column name\n",
    "        y_label = y\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(dataframe[x], dataframe[y])\n",
    "    slope_plt = hv.Slope(slope, intercept).opts(xlabel=x_label, ylabel=y_label, line_color=color, line_width=line_width, alpha=alpha, height=height, width=width)\n",
    "    return slope_plt, r_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf11be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidenceinterval(\n",
    "        dataframe: pd.DataFrame,  # dataframe\n",
    "        x: str,  # string with column name, used to determine confidence interval\n",
    "        y: str,  # string with column name, used to determine confidence interval\n",
    "        x_label: str = 'default',  # axis label to be printed on plot (does not need to match dataframe name)\n",
    "        x_range: tuple = None,  # range of x-axis\n",
    "        y_label: str = 'default',  # axis label to be printed on plot (does not need to match dataframe name)\n",
    "        y_range: tuple = None,  # range of y-axis\n",
    "        ci: int = 0.999,  # confidence interval (0.9-0.99 recommended)\n",
    "        color: str = '#5289a1',  # color of confidence interval\n",
    "        outline: str = '#FFFFFF',  # color of confidence interval line\n",
    "        alpha: int = 0.2,  # transparency of confidence interval\n",
    "        height: int = 500,  #plot height (recommended: 500)\n",
    "        width: int = 500  #plot width (recommended: 500)\n",
    "):\n",
    "        \n",
    "    \"\"\" \n",
    "    Confidence interval calculations use inferences made on the mean and variance of the distributed data (assumes normal distribution)\n",
    "    and is calculated by applying a student-t test. Plotting function based off of HoloViews 'Area' element as 'area between curves'. \n",
    "    See documentation for more information:\n",
    "    hv.help(hv.Area)\n",
    "    https://holoviews.org/reference/elements/bokeh/Area.html\n",
    "    \n",
    "    \"\"\"  \n",
    "\n",
    "    if x_label == 'default':  # if no x_label provided, use x column name\n",
    "        x_label = x\n",
    "    if y_label == 'default':  # if no y_label provided, use y column name\n",
    "        y_label = y\n",
    "\n",
    "    if not x_range:\n",
    "        x_min = min(dataframe[x]); x_max = max(dataframe[x])\n",
    "        x_buffer = abs(x_max-x_min)/10\n",
    "        x_range = (x_min-x_buffer, x_max+x_buffer)\n",
    "    if not y_range:\n",
    "        y_min = min(dataframe[y]); y_max = max(dataframe[y])\n",
    "        y_buffer = abs(y_max-y_min)/10\n",
    "        y_range = (y_min-y_buffer, y_max+y_buffer)\n",
    "\n",
    "    n = len(dataframe[x])\n",
    "    t_value = stats.t.ppf(1 - (1 - ci) / 2, n - 2)  # t-value for confidence interval (student-t test for n-2 degrees of freedom)\n",
    "    x_mean = np.mean(dataframe[x])  # mean of x values\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(dataframe[x], dataframe[y])\n",
    "\n",
    "    S_xx = (n * np.sum(dataframe[x] ** 2) - np.sum(dataframe[x]) ** 2) / n  # sample-corrected sum of squares (sum of the square of the difference between x and its mean)\n",
    "    S_xy = (n * np.sum(dataframe[x] * dataframe[y]) - np.sum(dataframe[x]) * np.sum(dataframe[y])) / n  # sample-corrected covariance for x and y \n",
    "    S_yy = (n * np.sum(dataframe[y] ** 2) - np.sum(dataframe[y]) ** 2) / n  # sample-corrected sum of squares (sum of the square of the difference between y and its mean)\n",
    "    \n",
    "    SSE = S_yy - slope * S_xy # sum of squared estimate of errors (deviation of the observed value from the estimated value)\n",
    "    s2 = SSE / (n - 2)  #variance of the x, y data\n",
    "    s = np.sqrt(s2)  # standard deviation of the x, y data\n",
    "\n",
    "    unique_x = np.unique(dataframe[x])  # unique x values (prevents overplotting of confidence interval)\n",
    "    mean_upperconfidence_list = slope * unique_x + intercept + t_value * s * np.sqrt((1 / n + (np.square(unique_x - x_mean)) / S_xx))  # line for upper confidence interval\n",
    "    mean_lowerconfidence_list = slope * unique_x + intercept - t_value * s * np.sqrt((1 / n + (np.square(unique_x - x_mean)) / S_xx))  # line for lower confidence interval\n",
    "\n",
    "    upper_spread = interp1d(x=unique_x, y=mean_upperconfidence_list, kind='quadratic', fill_value='extrapolate')  # interpolation function for upper confidence interval (smooths line)\n",
    "    lower_spread = interp1d(x=unique_x, y=mean_lowerconfidence_list, kind='quadratic', fill_value='extrapolate')  # interpolation function for lower confidence interval (smooths line)\n",
    "\n",
    "    ci_x = np.linspace(min(unique_x) - abs(max(unique_x) - min(unique_x)) / 2, max(unique_x) + abs(max(unique_x) - min(unique_x)) / 2, num=1000)  # x values for confidence interval plot (extends beyond data range)\n",
    "    ci_upper_y = upper_spread(ci_x)  # y values for upper confidence interval plot corresponding to 'extended' x values\n",
    "    ci_lower_y = lower_spread(ci_x)  # y values for lower confidence interval plot corresponding to 'extended' x values\n",
    "\n",
    "    # plot confidence interval\n",
    "    ci_plt = hv.Area((ci_x, ci_upper_y, ci_lower_y), vdims=['ci_y1', 'ci_y2']).opts(xlabel=x_label, ylabel=y_label, color=color, alpha=alpha, line_color=outline, height=height, width=width, xlim=x_range, ylim=y_range)\n",
    "    return ci_plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd50549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_graph(\n",
    "    dataframe: pd.DataFrame,\n",
    "    x: str,  # string with column name, used to determine x-axis\n",
    "    y: str,  # string with column name, used to determine y-axis\n",
    "    x_label: str = 'default',  # axis label to be printed on plot (does not need to match dataframe name)\n",
    "    y_label: str = 'default',  # axis label to be printed on plot (does not need to match dataframe name)\n",
    "\n",
    "    title: str = 'default',  # title of plot\n",
    "    discrete_x: bool = False,  # if True, will create a bar graph with discrete x-axis\n",
    "    svgs: str = None,  # string with column name of svgs \n",
    "    hover_list: list = None,  # list of column names with data to be shown on hover \n",
    "    color: str = '#5289a1',  # color of bars\n",
    "    alpha: int = 1,  # transparency of bars\n",
    "    height: int = 500,  #plot height (recommended: 500)\n",
    "    width: int = 500  #plot width (recommended: 500)\n",
    "):\n",
    "    \n",
    "    \"\"\" \n",
    "    bar_graph function (if continuous x-axis) based off of HoloViews 'Histogram' element. See documentation for more information:\n",
    "    hv.help(hv.Histogram)\n",
    "    http://dev.holoviews.org/reference/elements/bokeh/Histogram.html\n",
    "\n",
    "    for non-continuous x-axis, bar_graph function is based on 'hv.Bars' element. See documentation for more information:\n",
    "    hv.help(hv.Bars)\n",
    "    http://dev.holoviews.org/reference/elements/bokeh/Bars.html\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if x_label == 'default':  # if no x_label provided, use x column name\n",
    "        x_label = x\n",
    "    if y_label == 'default':  # if no y_label provided, use y column name\n",
    "        y_label = y\n",
    "    if title == 'default':  # if no title provided, define from x, y labels\n",
    "        title = f'{y_label} vs. {x_label}'\n",
    "\n",
    "    \n",
    "    if discrete_x == False:  # continuous x-axis, use Histogram element\n",
    "        if svgs == None and labels == None:\n",
    "            plt = hv.Histogram(dataframe, kdims=[x], vdims=[y]).opts(xlabel=x_label, ylabel=y_label, title=title, color=color, alpha=alpha, height=height, width=width)\n",
    "        else: \n",
    "            hover_list.insert(0, y)\n",
    "            tooltips = f'<div>end' # beginning of tooltips if no svgs provided\n",
    "            if svgs != None:\n",
    "                tooltips = f'<div><div>@{svgs}{{safe}}</div>end'  # beginning of tooltips if svgs are provided\n",
    "                hover_list.insert(1, svgs)\n",
    "            if len(hover_list) < 4:\n",
    "                for label in hover_list:\n",
    "                    if label != svgs and label != y:\n",
    "                        tooltips = tooltips.replace('end', f'<div><span style=\"font-size: 17px; font-weight: bold;\">@{label}</span></div>end')\n",
    "            else:\n",
    "                for label in hover_list:\n",
    "                    if label != svgs and label != y:\n",
    "                        tooltips = tooltips.replace('end', f'<div><span style=\"font-size: 12px;\">{label}: @{label}</span></div>end')\n",
    "            \n",
    "            tooltips = tooltips.replace('end', '</div>')\n",
    "            hover = HoverTool(tooltips=tooltips)\n",
    "            plt = hv.Histogram(dataframe, kdims=[x], vdims=hover_list).opts(xlabel=x_label, ylabel=y_label, title=title, tools=[hover], color=color, alpha=alpha, height=height, width=width)\n",
    "    else:  # discrete x-axis, use Bars element\n",
    "        if svgs == None and labels == None:\n",
    "            plt = hv.Bars(dataframe, kdims=[x], vdims=[y]).opts(xlabel=x_label, ylabel=y_label, title=title, color=color, alpha=alpha, height=height, width=width)\n",
    "        else: \n",
    "            hover_list.insert(0, y)\n",
    "            tooltips = f'<div>end' # beginning of tooltips if no svgs provided\n",
    "            if svgs != None:\n",
    "                tooltips = f'<div><div>@{svgs}{{safe}}</div>end'  # beginning of tooltips if svgs are provided\n",
    "                hover_list.insert(1, svgs)\n",
    "            if len(hover_list) < 4:\n",
    "                for label in hover_list:\n",
    "                    if label != svgs and label != y:\n",
    "                        tooltips = tooltips.replace('end', f'<div><span style=\"font-size: 17px; font-weight: bold;\">@{label}</span></div>end')\n",
    "            else:\n",
    "                for label in hover_list:\n",
    "                    if label != svgs and label != y:\n",
    "                        tooltips = tooltips.replace('end', f'<div><span style=\"font-size: 12px;\">{label}: @{label}</span></div>end')\n",
    "            \n",
    "            tooltips = tooltips.replace('end', '</div>')\n",
    "            hover = HoverTool(tooltips=tooltips)\n",
    "            plt = hv.Bars(dataframe, kdims=[x], vdims=hover_list).opts(xlabel=x_label, ylabel=y_label, title=title, tools=[hover], color=color, alpha=alpha, height=height, width=width, xlim=(min(dataframe[x]), max(dataframe[x])), ylim=(min(dataframe[y]), max(dataframe[y])))\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629d45d",
   "metadata": {},
   "source": [
    "## Chemical Space Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576bb039",
   "metadata": {},
   "source": [
    "### k-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_score(dataframe, k):\n",
    "    %matplotlib inline\n",
    "    \n",
    "    elbow_plot = KElbowVisualizer(KMeans(n_clusters=k, n_init='auto'), random_state=42)\n",
    "    elbow_plot.fit(dataframe)\n",
    "    return elbow_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c3064",
   "metadata": {},
   "source": [
    "## Sanitize Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221cb6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_column_names(df):\n",
    "    df.columns = df.columns.str.replace('[^a-zA-Z0-9]', '_')  # replace non-alphanumeric characters with '_'\n",
    "    df.columns = df.columns.str.replace('[ ,-]', '_', regex=True)  # replace spaces, commas, and hyphens with '_'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417692d4",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdccd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'InteractivePlots - MLR.xlsx'\n",
    "sheet = 'Sheet1'\n",
    "header = 0  # row number of header (0-index) (set header = 1 to drop row with x1, x2... column names if present)\n",
    "\n",
    "id_column = 'ID' #name or 0-index\n",
    "smiles_column = 'SMILES' #name or 0-index (leave blank if not available)\n",
    "response_column = 'Yield' #name or 0-index (leave blank if not available)\n",
    "descriptor_start_column = 'bite_angle' #name or 0-index\n",
    "\n",
    "# Read in data\n",
    "if file.endswith('.csv'):\n",
    "    df = pd.read_csv(file, header=header)\n",
    "elif file.endswith('.xlsx'):\n",
    "    df = pd.read_excel(file, sheet, header=header, engine='openpyxl')\n",
    "else:\n",
    "    print('File type not supported. Please use .csv, .xlsx, or .pkl file types.')\n",
    "\n",
    "# # Drop rows with NaN values (will remove smiles column if there are any missing fields)\n",
    "# df = df.dropna(axis=1, how='any')  \n",
    "\n",
    "# Generate list of descriptors \n",
    "descriptor_start_column_loc, descriptor_start_column = get_column_loc(descriptor_start_column, df)\n",
    "descriptors = list(df.columns)[descriptor_start_column_loc:]\n",
    "\n",
    "#  Generate folder for any saved figures, named with run date\n",
    "run_date = date.today().strftime(\"%b-%d-%Y\")\n",
    "if not os.path.exists(run_date):\n",
    "    os.makedirs(run_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21004067",
   "metadata": {},
   "source": [
    "## Generate Image from SMILES String"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbeaa0d",
   "metadata": {},
   "source": [
    "Note: RDKit image generation will occasionally produce overlapping groups (primarily for large structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d128e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "image_column = 'Image' #name of column that svgs will go in to (not pre-existing)\n",
    "\n",
    "smiles_column_loc, smiles_column = get_column_loc(smiles_column, dataframe)\n",
    "df = DrawMol(dataframe, smiles_column_loc, image_column)\n",
    "svgs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5266e72",
   "metadata": {},
   "source": [
    "# Univariate Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77076b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "y_axis = response_column\n",
    "threshold = 0.5\n",
    "hover_list = [id_column]\n",
    "\n",
    "plots_to_show = 2  # maximum number of plots to show (more than 4 may be difficult to see with this format)\n",
    "\n",
    "plt = ''; univariate_correlations = []; plot_count = 0\n",
    "for descriptor in descriptors:\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(dataframe[descriptor], dataframe[y_axis])\n",
    "    if r_value**2 > threshold:\n",
    "        univariate_correlations.append([[r_value**2], descriptor])\n",
    "        plot_count += 1\n",
    "        if plot_count <= plots_to_show:\n",
    "            print(f'{descriptor}: R^2 = {round(r_value**2, 2)}, p value = {p_value:.3}')\n",
    "            scatter_plt = scatter_plot(dataframe, x=descriptor, y=y_axis, svgs=image_column, hover_list=hover_list, color='#7291ab', alpha=0.8)\n",
    "            slope, r_value = plot_slope(dataframe, x=descriptor, y=y_axis)\n",
    "            confidence_interval = plot_confidenceinterval(dataframe, x=descriptor, y=y_axis)  # Planning to change confidence interval to prediction interval, more meaningful (also ci may be too narrow than what it should be)\n",
    "\n",
    "            subplt = scatter_plt * slope * confidence_interval\n",
    "\n",
    "            if plt: \n",
    "                plt = plt + subplt\n",
    "            else:\n",
    "                plt = subplt\n",
    "\n",
    "univariate_correlations = sorted(univariate_correlations, key=lambda x: x[0], reverse=True); print(f'{len(univariate_correlations)} descriptors with R^2 > {threshold}')\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f3391f",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15fd419",
   "metadata": {},
   "source": [
    "## Remove Colinear Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301788c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "threshold = 0.9  # threshold for colinearity (1 = perfect colinearity)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = dataframe[descriptors].corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than threshold\n",
    "columns_to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "print(f'{len(descriptors)} descriptors before colinearity cutoff.\\n{len(columns_to_drop)} descriptors removed.\\n{len(descriptors)-len(columns_to_drop)} remaining.')\n",
    "\n",
    "# Drop columns with high correlation\n",
    "dataframe.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Regenerate list of descriptors\n",
    "descriptors = list(dataframe.columns)[descriptor_start_column_loc+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72668cbc",
   "metadata": {},
   "source": [
    "## Withold Points for External Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc4e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "\n",
    "split_type = 'random' # 'random', 'defined', or 'none'\n",
    "\n",
    "external_validation_IDs = [1, 2, 3]  # list of external set IDs (if split_type = 'defined')\n",
    "split_size = 0  # ratio of points for test set (if split_type = 'random') (0.1 means 10% of data will be used for external validation)\n",
    "\n",
    "# Split data into training and test sets\n",
    "if split_type == 'defined':\n",
    "    train = dataframe[dataframe[id_column].isin(external_validation_IDs)]\n",
    "    external_validation = dataframe[~dataframe[id_column].isin(external_validation_IDs)]\n",
    "if split_type == 'random':\n",
    "    train, external_validation = train_test_split(dataframe, test_size=split_size, random_state=33)\n",
    "if split_type == 'none':\n",
    "    train = dataframe\n",
    "    external_validation = pd.DataFrame()\n",
    "\n",
    "# Add split column to dataframes\n",
    "train['Split'] = 'train'; external_validation['Split'] = 'external validation'\n",
    "df = pd.concat([train, external_validation]).sort_index()\n",
    "\n",
    "print(f'{len(train)} points held for training/internal validation set.\\n{len(external_validation)} points in external validation set.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892bfe3",
   "metadata": {},
   "source": [
    "## Designate Training and Internal Validation Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39560ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df[df['Split'] != 'external validation']\n",
    "\n",
    "split_type = 'random' # 'random', 'defined', or 'none'\n",
    "\n",
    "internal_validation_IDs = [1, 2, 3]  # list of external set IDs (if split_type = 'defined')\n",
    "split_size = 0.2  # ratio of points for test set (if split_type = 'random') (0.1 means 10% of data will be used for external validation)\n",
    "\n",
    "# Split data into training and test sets\n",
    "if split_type == 'defined':\n",
    "    train = dataframe[dataframe[id_column].isin(internal_validation_IDs)]\n",
    "    internal_validation = dataframe[~dataframe[id_column].isin(internal_validation_IDs)]\n",
    "if split_type == 'random':\n",
    "    train, internal_validation = train_test_split(dataframe, test_size=split_size, random_state=40)\n",
    "if split_type == 'none':\n",
    "    train = dataframe\n",
    "    internal_validation = pd.DataFrame()\n",
    "\n",
    "# Add split column to dataframes\n",
    "train['Split'] = 'train'; internal_validation['Split'] = 'internal validation'\n",
    "df = pd.concat([train, internal_validation, external_validation]).sort_index()\n",
    "\n",
    "print(f'{len(train)} points in training set.\\n{len(internal_validation)} points in internal validation set.')\n",
    "if len(external_validation) > 0:\n",
    "    print(f'{len(external_validation)} points in external validation set.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333f8f2",
   "metadata": {},
   "source": [
    "## View Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data splits against response column (if defined)\n",
    "dataframe = df\n",
    "x_axis = response_column\n",
    "\n",
    "if x_axis:\n",
    "    colors = ['#34c0eb', '#1a1617', '#ad284e']\n",
    "    n_bins = 8 # number of bins for histogram (adjust as needed)\n",
    "    \n",
    "    bins = [min(dataframe[x_axis]) + x * (max(dataframe[x_axis]) - min(dataframe[x_axis]))/n_bins for x in range(0, n_bins)]\n",
    "    plt = hv.NdOverlay({split: hv.Histogram(np.histogram(dataframe.loc[dataframe['Split'] == split][x_axis], bins=bins)).opts(color=colors[count], alpha=0.5) for count, split in enumerate(dataframe['Split'].unique())}).opts(width=500, height=500)\n",
    "    \n",
    "\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b139149",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a4376c",
   "metadata": {},
   "source": [
    "Note: currently gives 'FutureWarning' due to dataframe use, will be updated (error goes away when cell is run twice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef775d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "scaler = 'Standard'\n",
    "\n",
    "if scaler == 'Standard':\n",
    "    scaler = StandardScaler()\n",
    "if scaler == 'MinMax':\n",
    "    scaler = MinMaxScaler()\n",
    "if scaler == 'Quantile':\n",
    "    scaler = QuantileTransformer(random_state=3)\n",
    "if scaler == 'Robust':\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "# Scale descriptors\n",
    "\n",
    "if 'Split' in dataframe.columns.to_list():\n",
    "    dataframe.loc[dataframe['Split'] == 'train', descriptors] = scaler.fit_transform(dataframe.loc[dataframe['Split'] == 'train', descriptors])  # fit and transform the training set\n",
    "    if 'internal validation' in list(dataframe['Split']):\n",
    "        dataframe.loc[dataframe['Split'] == 'internal validation', descriptors] = scaler.transform(dataframe.loc[dataframe['Split'] == 'internal validation', descriptors])  # transform the internal validation set\n",
    "    if 'external validation' in list(dataframe['Split']):\n",
    "        dataframe.loc[dataframe['Split'] == 'external validation', descriptors] = scaler.transform(dataframe.loc[dataframe['Split'] == 'external validation', descriptors])  # transform the external validation set\n",
    "else:\n",
    "    dataframe[descriptors] = scaler.fit_transform(dataframe[descriptors])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f3804e",
   "metadata": {},
   "source": [
    "# Chemical Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8d865",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ab012",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "n_components = 10  # use n_components = 'mle' for automated solving (uses Minka's MLE), requires more samples than features\n",
    "# number of principal components to keep (principal components = \"axes\", recommended to describe ~60-80% of variance)\n",
    "# number of principal components must be larger than the number of samples or the number of features (whichever is smaller)\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(dataframe[descriptors])\n",
    "principal_components = pca.transform(dataframe[descriptors])\n",
    "\n",
    "# Add principal components to dataframe\n",
    "if n_components != 'mle':\n",
    "    for i in range(n_components):\n",
    "        dataframe[f'pc{i+1}'] = principal_components[:, i]\n",
    "\n",
    "# Print explained variance\n",
    "pca_score = pca.explained_variance_ratio_\n",
    "print(f'{round(np.sum(pca_score)*100, 1)}% of variance explained by {n_components} principal components\\n')\n",
    "print(f'Variance explained by each principal component:')\n",
    "for i, variance in enumerate(pca_score):\n",
    "    print(f'pc{i+1}: {round(variance*100, 1)}%')\n",
    "\n",
    "for index, row in dataframe.iterrows():\n",
    "    for descriptor in descriptors:\n",
    "        dataframe.at[index, f'{descriptor}_pc'] = row[descriptor] * pca.components_[0][descriptors.index(descriptor)]\n",
    "        dataframe.at[index, f'{descriptor}_pc_weight'] = pca.components_[0][descriptors.index(descriptor)]\n",
    "\n",
    "dataframe.to_csv(f'{run_date}/PCA.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4626db",
   "metadata": {},
   "source": [
    "### View PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "x_axis = 'pc1'\n",
    "y_axis = 'pc2'\n",
    "hover_list = [id_column]\n",
    "\n",
    "save_plot = False\n",
    "file_name = 'pca'\n",
    "file_path = run_date\n",
    "\n",
    "plt = scatter_plot(dataframe, x=x_axis, y=y_axis, svgs=image_column, hover_list=hover_list, color='#7291ab', alpha=0.8)\n",
    "\n",
    "if save_plot:\n",
    "    hv.save(plt, file_path + '/' + file_name + '.html', fmt='html')\n",
    "\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1dea1",
   "metadata": {},
   "source": [
    "## Cluster PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526284c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "dimred_columns = [col for col in dataframe.columns if col.startswith('pc')]\n",
    "k = (3, 10)\n",
    "\n",
    "clustering_algorithm = 'kMeans'\n",
    "\n",
    "if clustering_algorithm == 'kMeans':\n",
    "    elbow_method = kmeans_score(dataframe[dimred_columns], k)\n",
    "    n_clust = elbow_method.elbow_value_\n",
    "    print(f'Optimal number of clusters using distortion score (elbow plot): {n_clust}')\n",
    "    elbow_method.show()\n",
    "\n",
    "dimred_columns = [col for col in dataframe.columns if col.startswith('pc')]\n",
    "\n",
    "kmeans_clustering = KMeans(n_clusters=n_clust, random_state=42, n_init=10)\n",
    "kmeans_clustering.fit(dataframe[dimred_columns])\n",
    "\n",
    "dataframe[f'{clustering_algorithm}_cluster'] = kmeans_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbbec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "x_axis = 'pc1'\n",
    "y_axis = 'pc2'\n",
    "\n",
    "groupby = 'kMeans_cluster'  # name or 0-index for column to group data by\n",
    "color = 'Category20'  # color palette for grouped data\n",
    "\n",
    "save_plot = False\n",
    "file_name = 'Clustered PCA Space'\n",
    "file_path = run_date\n",
    "\n",
    "plt = ''\n",
    "plt = scatter_plot(dataframe, x=x_axis, y=y_axis, svgs=image_column, hover_list=[id_column], groupby=groupby, color=color, outline=color)\n",
    "\n",
    "if save_plot:\n",
    "    hv.save(plt, file_path + '/' + file_name + '.html', fmt='html')\n",
    "\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7889452",
   "metadata": {},
   "source": [
    "# MLR Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13237ef9",
   "metadata": {},
   "source": [
    "Note: best to incorporate additional validation scores (LOO, k-fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a2ac5",
   "metadata": {},
   "source": [
    "## First Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c08c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df[df['Split'] != 'external validation'].copy()\n",
    "y_axis = response_column\n",
    "threshold = 0.2  # threshold for features to retain (recommended to keep low unless you have many features/plan to run many steps)\n",
    "hover_list = [id_column]\n",
    "\n",
    "first_step_results = []  # list to store results of first step of MLR (formatted as [Training R^2, Training MAE, Validation R^2, Validation MAE], descriptor)\n",
    "regression = linear_model.LinearRegression()\n",
    "for d1 in descriptors:\n",
    "    mlr_features = [d1]\n",
    "    if 'internal validation' in list(dataframe['Split']):\n",
    "        regression.fit(dataframe.loc[dataframe['Split'] == 'train', mlr_features], dataframe.loc[dataframe['Split'] == 'train', response_column])\n",
    "    else:\n",
    "        regression.fit(dataframe[mlr_features], dataframe[response_column])\n",
    "\n",
    "    pred_y = regression.predict(dataframe[mlr_features])\n",
    "    dataframe.loc[:, 'mlr prediction'] = pred_y\n",
    "    train_r2 = r2_score(dataframe.loc[dataframe['Split'] == 'train', response_column], dataframe.loc[dataframe['Split'] == 'train', response_column], dataframe.loc[dataframe['Split'] == 'train', 'mlr prediction'])\n",
    "    \n",
    "    if train_r2 > threshold:\n",
    "        validation_r2 = r2_score(dataframe.loc[dataframe['Split'] == 'train', response_column], dataframe.loc[dataframe['Split'] == 'internal validation', response_column], dataframe.loc[dataframe['Split'] == 'internal validation', 'mlr prediction'])\n",
    "        train_mae = mean_absolute_error(dataframe.loc[dataframe['Split'] == 'train', response_column], dataframe.loc[dataframe['Split'] == 'train', 'mlr prediction'])\n",
    "        \n",
    "        if 'internal validation' in list(dataframe['Split']):\n",
    "            validation_mae = mean_absolute_error(dataframe.loc[dataframe['Split'] == 'internal validation', response_column], dataframe.loc[dataframe['Split'] == 'internal validation', 'mlr prediction'])\n",
    "            first_step_results.append([[train_r2, train_mae, validation_r2, validation_mae], d1])\n",
    "        else: \n",
    "            validation_mae = None; first_step_results = None\n",
    "\n",
    "first_step_results = sorted(first_step_results, key=lambda x: x[0][0], reverse=True); print(f'{len(first_step_results)} descriptors with R^2 > {threshold}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ff9dfa",
   "metadata": {},
   "source": [
    "### Sort Results from First Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d79c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by = 'train r2'  # options are 'train r2', 'train mae', 'validation r2', 'validation mae'\n",
    "models_to_print = 5  # number of models to print\n",
    "\n",
    "# Define the sorting options and their respective indices and orders\n",
    "sort_options = {\n",
    "    'train r2': (0, 0, True),\n",
    "    'train mae': (0, 1, False),\n",
    "    'validation r2': (0, 2, True),\n",
    "    'validation mae': (0, 3, False)\n",
    "}\n",
    "\n",
    "# Get the sorting parameters based on the selected option\n",
    "sort_index, sort_subindex, sort_reverse = sort_options[sort_by]\n",
    "\n",
    "# Sort the results based on the selected option\n",
    "first_step_results = sorted(first_step_results, key=lambda x: x[sort_index][sort_subindex], reverse=sort_reverse)\n",
    "\n",
    "# Print the top models\n",
    "for model_stats, features in first_step_results[:models_to_print]:\n",
    "    print(f'{features}:\\n\\tTraining R^2 = {model_stats[0]:.2f}, Validation R^2 = {model_stats[2]:.2f}\\n\\tTraining MAE = {model_stats[1]:.2f}, Validation MAE = {model_stats[3]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d7696e",
   "metadata": {},
   "source": [
    "## Second Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184918e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df[df['Split'] != 'external validation'].copy()\n",
    "y_axis = response_column\n",
    "threshold = 0.2  # threshold for features to retain (recommended to keep low unless you have many features/plan to run many steps)\n",
    "hover_list = [id_column]\n",
    "\n",
    "second_step_results = []  # list to store results of first step of MLR (formatted as [Training R^2, Training MAE, Validation R^2, Validation MAE], descriptor)\n",
    "regression = linear_model.LinearRegression()\n",
    "for model_stats, d1 in first_step_results:\n",
    "    for d2 in descriptors:\n",
    "        if d1 != d2:\n",
    "            mlr_features = [d1, d2]\n",
    "            if 'internal validation' in list(dataframe['Split']):\n",
    "                regression.fit(dataframe.loc[dataframe['Split'] == 'train', mlr_features], dataframe.loc[dataframe['Split'] == 'train', response_column])\n",
    "            else:\n",
    "                regression.fit(dataframe[mlr_features], dataframe[response_column])\n",
    "\n",
    "            pred_y = regression.predict(dataframe[mlr_features])\n",
    "            dataframe.loc[:, 'mlr prediction'] = pred_y\n",
    "            train_r2 = r2_score(dataframe.loc[dataframe['Split'] == 'train', response_column], dataframe.loc[dataframe['Split'] == 'train', response_column], dataframe.loc[dataframe['Split'] == 'train', 'mlr prediction'])\n",
    "            \n",
    "            if train_r2 > threshold:\n",
    "                train_mae = mean_absolute_error(dataframe.loc[dataframe['Split'] == 'train', response_column], dataframe.loc[dataframe['Split'] == 'train', 'mlr prediction'])\n",
    "                \n",
    "                if 'internal validation' in list(dataframe['Split']):\n",
    "                    validation_r2 = r2_score(dataframe.loc[dataframe['Split'] == 'train', response_column], dataframe.loc[dataframe['Split'] == 'internal validation', response_column], dataframe.loc[dataframe['Split'] == 'internal validation', 'mlr prediction'])\n",
    "                    validation_mae = mean_absolute_error(dataframe.loc[dataframe['Split'] == 'internal validation', response_column], dataframe.loc[dataframe['Split'] == 'internal validation', 'mlr prediction'])\n",
    "                else: \n",
    "                    validation_r2 = None; validation_mae = None\n",
    "                \n",
    "                second_step_results.append([[train_r2, train_mae, validation_r2, validation_mae], d1, d2])\n",
    "\n",
    "second_step_results = sorted(second_step_results, key=lambda x: x[0][0], reverse=True); print(f'{len(second_step_results)} two parameter models with R^2 > {threshold}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e31295",
   "metadata": {},
   "source": [
    "### Sort Results from Second Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by = 'validation r2'  # options are 'train r2', 'train mae', 'validation r2', 'validation mae'\n",
    "models_to_print = 5  # number of models to print\n",
    "\n",
    "# Define the sorting options and their respective indices and orders\n",
    "sort_options = {\n",
    "    'train r2': (0, 0, True),\n",
    "    'train mae': (0, 1, False),\n",
    "    'validation r2': (0, 2, True),\n",
    "    'validation mae': (0, 3, False)\n",
    "}\n",
    "\n",
    "# Get the sorting parameters based on the selected option\n",
    "sort_index, sort_subindex, sort_reverse = sort_options[sort_by]\n",
    "\n",
    "# Sort the results based on the selected option\n",
    "second_step_results = sorted(second_step_results, key=lambda x: x[sort_index][sort_subindex], reverse=sort_reverse)\n",
    "\n",
    "# Print the top models\n",
    "for model_stats, d1, d2 in second_step_results[:models_to_print]:\n",
    "    print(f'{d1}, {d2}:\\n\\tTraining R^2 = {model_stats[0]:.2f}, Validation R^2 = {model_stats[2]:.2f}\\n\\tTraining MAE = {model_stats[1]:.2f}, Validation MAE = {model_stats[3]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde9bfc",
   "metadata": {},
   "source": [
    "## Visualize MLR Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a289f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df[df['Split'] != 'external validation'].copy()\n",
    "features = ['Vbur%_3.0_Ang', 'aniso_P_NMR_min']\n",
    "y_axis = response_column\n",
    "hover_list = [id_column]  # list of column names with data to be shown on hover\n",
    "\n",
    "\n",
    "save_plot = False\n",
    "file_name = f'{response_column} vs. {\", \".join(features)}'\n",
    "file_path = run_date\n",
    "\n",
    "x_label = f'{response_column}'\n",
    "y_label = f'Predicted {response_column}'\n",
    "title = f'{y_label} vs {x_label}'\n",
    "\n",
    "regression = linear_model.LinearRegression()\n",
    "regression.fit(dataframe[features], dataframe[response_column])\n",
    "dataframe['mlr prediction'] = regression.predict(dataframe[features])\n",
    "coefficients = regression.coef_; intercept = regression.intercept_\n",
    "\n",
    "terms = [f'({round(coef, 3)} * {desc})' for coef, desc in zip(coefficients, features)]\n",
    "model_equation = f'{response_column} = {round(regression.intercept_, 3)} + ' + ' ' + ' + '.join(terms)\n",
    "\n",
    "all_scatter_plt = scatter_plot(dataframe, x=response_column, y='mlr prediction', svgs=image_column, hover_list=hover_list, x_label=x_label, y_label=y_label, title=title, color='#7291ab', alpha=0)\n",
    "train_scatter_plt = scatter_plot(dataframe[dataframe['Split']=='train'], x=response_column, y='mlr prediction', svgs=image_column, hover_list=hover_list, x_label=x_label, y_label=y_label, title=title, color='#60a9e6', alpha=0.8)\n",
    "if 'internal validation' in list(dataframe['Split']):\n",
    "    validation_scatter_plt = scatter_plot(dataframe[dataframe['Split']=='internal validation'], x=response_column, y='mlr prediction', svgs=image_column, hover_list=hover_list, x_label=x_label, y_label=y_label, title=title, color='#0d263b', alpha=0.8)\n",
    "else:\n",
    "    validation_scatter_plt = all_scatter_plt\n",
    "\n",
    "slope, r_value = plot_slope(dataframe[dataframe['Split']=='train'], x=response_column, y='mlr prediction', x_label=x_label, y_label=y_label)\n",
    "confidence_interval = plot_confidenceinterval(dataframe, x=response_column, y='mlr prediction')  # Planning to change confidence interval to prediction interval, more meaningful (also ci may be too narrow than what it should be)\n",
    "\n",
    "train_r2 = r2_score(dataframe.loc[dataframe['Split'] == 'train', response_column], dataframe.loc[dataframe['Split'] == 'train', response_column], dataframe.loc[dataframe['Split'] == 'train', 'mlr prediction'])\n",
    "train_mae = mean_absolute_error(dataframe.loc[dataframe['Split'] == 'train', response_column], dataframe.loc[dataframe['Split'] == 'train', 'mlr prediction'])            \n",
    "                \n",
    "if 'internal validation' in list(dataframe['Split']):\n",
    "    validation_r2 = r2_score(dataframe.loc[dataframe['Split'] == 'train', response_column], dataframe.loc[dataframe['Split'] == 'internal validation', response_column], dataframe.loc[dataframe['Split'] == 'internal validation', 'mlr prediction'])\n",
    "    validation_mae = mean_absolute_error(dataframe.loc[dataframe['Split'] == 'internal validation', response_column], dataframe.loc[dataframe['Split'] == 'internal validation', 'mlr prediction'])\n",
    "else: \n",
    "    validation_r2 = None; validation_mae = None\n",
    "\n",
    "print(model_equation)\n",
    "print(f'Training R^2 = {train_r2:.2f}, Validation R^2 = {validation_r2:.2f}')\n",
    "print(f'Training MAE = {train_mae:.2f}, Validation MAE = {validation_mae:.2f}')\n",
    "\n",
    "plt = train_scatter_plt * validation_scatter_plt * slope * confidence_interval\n",
    "plt = all_scatter_plt * plt\n",
    "\n",
    "if save_plot:\n",
    "    hv.save(plt, file_path + '/' + file_name + '.html', fmt='html')\n",
    "\n",
    "plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a429d",
   "metadata": {},
   "source": [
    "# Other Interactive Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e6110",
   "metadata": {},
   "source": [
    "### Categorical Coloring (user defined colors and labels, in contrast to cluster plotting method above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea9a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "x_axis = 'pc1'\n",
    "y_axis = 'pc2'\n",
    "\n",
    "category_column = 'R_Selective'  # name or 0-index for column containing category criteria\n",
    "category_criteria = {  # column value: [label, color] (column value can be string or integer)\n",
    "    0 : ['S Selective', '#a7c2db'],\n",
    "    1 : ['R Selective', '#fabc7a'],\n",
    "}\n",
    "\n",
    "save_plot = False\n",
    "file_name = 'Selectivity Data in PCA Space'\n",
    "file_path = run_date\n",
    "\n",
    "plt = None\n",
    "for key, value in category_criteria.items():\n",
    "    legend = value[0]\n",
    "    color = value[1]\n",
    "    if plt:\n",
    "        subplt = scatter_plot(dataframe.loc[dataframe[category_column] == key], x=x_axis, y=y_axis, legend=legend, svgs=image_column, hover_list=[id_column], color=color, outline=color, alpha=1)\n",
    "        plt = plt * subplt\n",
    "    else:\n",
    "        plt = scatter_plot(dataframe.loc[dataframe[category_column] == key], x=x_axis, y=y_axis, legend=legend, svgs=image_column, hover_list=[id_column], color=color, outline=color, alpha=1)\n",
    "base_plt = scatter_plot(dataframe, x=x_axis, y=y_axis, svgs=image_column, hover_list=[id_column], alpha=0)  # add svg hover info to all layers (default is bottom plt only)\n",
    "plt = base_plt * plt\n",
    "\n",
    "if save_plot:\n",
    "    hv.save(plt, file_path + '/' + file_name + '.html', fmt='html')\n",
    "\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581966a0",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "x_axis = 'pc1'\n",
    "y_axis = 'pc2'\n",
    "color_col = response_column\n",
    "heatmap_color = 'Plasma'\n",
    "title = f'{y_axis} vs {x_axis} colored by {color_col}'\n",
    "hover_list = [id_column]\n",
    "\n",
    "save_plot = False\n",
    "file_name = f'pca heatmap colored by {color_col}'\n",
    "file_path = run_date\n",
    "\n",
    "plt = scatter_plot(dataframe, x=x_axis, y=y_axis, title=title, svgs=image_column, hover_list=hover_list, heatmap=True, heatmap_col=color_col, heatmap_color=heatmap_color, alpha=0.8)\n",
    "\n",
    "\n",
    "if save_plot:\n",
    "    hv.save(plt, file_path + '/' + file_name + '.html', fmt='html')\n",
    "\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4338d6e5",
   "metadata": {},
   "source": [
    "### Bubble Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7883df4e",
   "metadata": {},
   "source": [
    "Note: this may have issues with size if using unscaled features (to be fixed in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe = df\n",
    "x_axis = 'pc1'\n",
    "y_axis = 'pc2'  \n",
    "size_col = 'Vbur%_3.0_Ang'  # name or 0-index for column to size data by\n",
    "title = f'PCA, sized by {color_col}'\n",
    "hover_list = [id_column]\n",
    "\n",
    "save_plot = False\n",
    "file_name = 'pca all'\n",
    "file_path = run_date\n",
    "\n",
    "plt = scatter_plot(dataframe, x=x_axis, y=y_axis, title=title, svgs=image_column, hover_list=hover_list, bubbleplot=True, bubblesize=size_col, alpha=0.8)\n",
    "\n",
    "\n",
    "if save_plot:\n",
    "    hv.save(plt, file_path + '/' + file_name + '.html', fmt='html')\n",
    "\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2c711",
   "metadata": {},
   "source": [
    "### Heatmap Bubble Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d09944",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "x_axis = 'pc1'\n",
    "y_axis = 'pc2'\n",
    "color_col = response_column\n",
    "heatmap_color = 'Plasma'\n",
    "size_col = 'Vbur%_3.0_Ang'\n",
    "hover_list = [id_column]\n",
    "\n",
    "save_plot = False\n",
    "file_name = 'pca all'\n",
    "file_path = run_date\n",
    "\n",
    "plt = scatter_plot(dataframe, x=x_axis, y=y_axis, svgs=image_column, hover_list=hover_list, heatmap=True, heatmap_col=color_col, heatmap_color=heatmap_color, bubbleplot=True, bubblesize=size_col, alpha=0.8)\n",
    "\n",
    "if save_plot:\n",
    "    hv.save(plt, file_path + '/' + file_name + '.html', fmt='html')\n",
    "\n",
    "plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
